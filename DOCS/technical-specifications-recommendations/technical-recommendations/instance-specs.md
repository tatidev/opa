# Technical Recommendations for OPMS-NetSuite API

## üéØ **OVERVIEW**

This document provides technical recommendations for hardware specifications, performance optimization, and scaling considerations for the OPMS-NetSuite API, specifically focused on large-scale CSV import operations.

**Last Updated**: September 1, 2025  
**Target Workload**: Up to 6,000 item NetSuite imports  
**Current Status**: ‚úÖ Tested and validated with 100% success rate

---

## üíæ **MEMORY (RAM) REQUIREMENTS**

### **Memory Usage Analysis for 6000 Items:**

| Component | Memory Usage | Description |
|-----------|--------------|-------------|
| **CSV File in Memory** | ~12 MB | 6000 items √ó 2KB average row size |
| **Transformed Payloads** | ~18 MB | 6000 items √ó 3KB NetSuite payload |
| **Mini-forms HTML Content** | ~30 MB | Rich HTML formatting per item |
| **Database Overhead** | ~50 MB | Connection pools, job tracking |
| **Node.js Runtime** | ~100 MB | Base process, modules, heap |
| **Progress Tracking** | ~10 MB | In-memory job state management |
| **TOTAL ESTIMATED** | **~220 MB** | Core processing requirements |
| **WITH SAFETY MARGIN** | **~350 MB** | 50% buffer for memory spikes |

### **RAM Recommendations by Scale:**

| Item Count | Minimum RAM | Recommended RAM | Notes |
|------------|-------------|-----------------|-------|
| **1-1,000** | 1 GB | 2 GB | Small batches, minimal overhead |
| **1,000-6,000** | 2 GB | **4-8 GB** | **Current target range** |
| **6,000-15,000** | 4 GB | 8-16 GB | Large batch processing |
| **15,000+** | 8 GB+ | 16 GB+ | Consider job chunking |

---

## ‚ö° **CPU REQUIREMENTS**

### **CPU Characteristics for NetSuite Import:**

| Processing Phase | CPU Pattern | Duration | Characteristics |
|------------------|-------------|----------|-----------------|
| **CSV Parsing** | Burst intensive | ~30 seconds | High CPU, short duration |
| **Data Transformation** | Moderate compute | ~2-3 minutes | Steady processing |
| **NetSuite API Calls** | Network I/O bound | ~1.75 hours | Low CPU, high network wait |
| **Database Operations** | I/O bound | Throughout | Minimal CPU usage |
| **Progress Tracking** | Background | Throughout | Negligible CPU |

### **CPU Recommendations by Scale:**

| Item Count | Minimum CPU | Recommended CPU | Optimal Instance Type | Monthly Cost (On-Demand) | Monthly Cost (Reserved) |
|------------|-------------|-----------------|----------------------|-------------------------|------------------------|
| **1-1,000** | 1 vCPU | 2 vCPU | t4g.small | ~$12/month | ~$8-10/month |
| **1,000-6,000** | 2 vCPU | **2-4 vCPU** | **t4g.large** | ~$49/month | ~$32-35/month |
| **6,000-15,000** | 2 vCPU | 4 vCPU | **t4g.xlarge** ‚úÖ | **~$98/month** | **~$65-70/month** |
| **15,000+** | 4 vCPU | 4-8 vCPU | Consider c6g instances | $150+/month | $100+/month |

---

## üèóÔ∏è **CURRENT INFRASTRUCTURE ASSESSMENT**

### **Deployed Configuration:**
- **Instance Type**: `t4g.xlarge` (ARM64 Graviton2) - **UPGRADED FOR ENTERPRISE SCALE**
- **CPU**: 4 vCPU  
- **RAM**: 16 GB
- **Auto Scaling**: 1-3 instances
- **Network**: Enhanced networking enabled

### **Capacity Analysis for 6000 Items:**

| Resource | Current | Required | Headroom | Status |
|----------|---------|----------|----------|---------|
| **RAM** | 16 GB | ~350 MB | **46x** | ‚úÖ **Excellent** |
| **CPU** | 4 vCPU | 1-2 vCPU | **2x** | ‚úÖ **Optimal** |
| **Storage** | 20 GB SSD | <1 GB | **20x** | ‚úÖ **Sufficient** |
| **Network** | Enhanced | High I/O | **Good** | ‚úÖ **Adequate** |

### **‚úÖ VERDICT: INFRASTRUCTURE UPGRADED FOR ENTERPRISE-SCALE PROCESSING**

---

## ‚è±Ô∏è **PROCESSING TIME ESTIMATES**

### **6000 Items Processing Timeline:**

| Configuration | Batch Size | Delay (ms) | Total Time | Efficiency |
|---------------|------------|------------|------------|------------|
| **Conservative** | 10 | 2000 | ~2 hours | Safe, low risk |
| **Balanced** | 15 | 1500 | ~1.75 hours | Good balance |
| **Optimized** | 20 | 1000 | ~1.5 hours | **Recommended** |
| **Aggressive** | 25 | 800 | ~1.25 hours | Higher risk |

### **Recommended Configuration:**
```bash
# Optimal settings for 6000 items
{
  "batchSize": 20,
  "delayMs": 1000,
  "dryRun": false
}
```

**Reasoning:**
- **20 items/batch**: Balances throughput with NetSuite stability
- **1000ms delay**: Respects NetSuite rate limits while maintaining speed
- **Expected time**: ~1.5 hours for 6000 items

---

## üöÄ **PERFORMANCE OPTIMIZATIONS**

### **Memory Optimizations:**
1. **Stream Processing**: For 15K+ items, consider streaming CSV parsing
2. **Garbage Collection**: Monitor Node.js heap usage during large imports
3. **Connection Pooling**: Optimize database connection pool size
4. **Payload Chunking**: Break large HTML content into smaller chunks

### **CPU Optimizations:**
1. **Async Processing**: Leverage Node.js event loop for I/O operations
2. **Batch Parallelization**: Process multiple items simultaneously within batches
3. **Worker Threads**: Consider worker threads for CPU-intensive transformations
4. **Caching**: Cache frequently used transformation logic

### **Network Optimizations:**
1. **Connection Reuse**: Maintain persistent connections to NetSuite
2. **Compression**: Enable gzip compression for API responses
3. **Retry Logic**: Implement exponential backoff for failed requests
4. **Circuit Breaker**: Prevent cascade failures during NetSuite outages

---

## üìä **SCALING MATRIX**

### **Horizontal Scaling (Multiple Instances):**

| Concurrent Jobs | Instances | Instance Type | Total Capacity |
|-----------------|-----------|---------------|----------------|
| **1 job** | 1 | t4g.large | 6,000 items |
| **2-3 jobs** | 2 | t4g.large | 12,000 items |
| **4-6 jobs** | 3 | t4g.large | 18,000 items |
| **6+ jobs** | 3 | t4g.xlarge | 30,000+ items |

### **Vertical Scaling (Larger Instances):**

| Instance Type | vCPU | RAM | Max Items | Use Case | Monthly Cost (On-Demand) | Monthly Cost (Reserved) |
|---------------|------|-----|-----------|----------|-------------------------|------------------------|
| **t4g.small** | 2 | 2 GB | 1,000 | Development/testing | ~$12 | ~$8-10 |
| **t4g.medium** | 2 | 4 GB | 3,000 | Small production | ~$25 | ~$16-18 |
| **t4g.large** | 2 | 8 GB | 6,000 | Development/QA | ~$49 | ~$32-35 |
| **t4g.xlarge** ‚úÖ | 4 | 16 GB | **15,000** | **Production target** | **~$98** | **~$65-70** |
| **t4g.2xlarge** | 8 | 32 GB | 30,000+ | Massive imports | ~$196 | ~$130-140 |

---

## üéØ **SPECIFIC RECOMMENDATIONS FOR 6000 ITEMS**

### **‚úÖ UPGRADED TO ENTERPRISE SCALE:**
Infrastructure upgraded to `t4g.xlarge` instances for production-scale 6000+ item imports:

- ‚úÖ **RAM**: 16GB provides 46x headroom over requirements
- ‚úÖ **CPU**: 4 vCPU enables faster processing and concurrent jobs  
- ‚úÖ **Performance**: Optimized for large-scale NetSuite imports
- ‚úÖ **Scalability**: Can handle multiple concurrent large jobs efficiently

### **üîß OPTIMAL IMPORT CONFIGURATION:**
```bash
# Production-ready settings for 6000 items
curl -X POST "https://api-dev.opuzen-service.com/api/netsuite/import/csv" \
  -F "file=@large-export-6000.csv" \
  -F 'options={"batchSize":20,"delayMs":1000,"dryRun":false}'
```

### **üìà MONITORING RECOMMENDATIONS:**
1. **CloudWatch Metrics**: Monitor CPU and memory utilization
2. **Job Duration**: Track import completion times
3. **Error Rates**: Monitor NetSuite API failure rates
4. **Database Performance**: Watch for connection pool exhaustion

---

## üö® **RISK MITIGATION**

### **High-Volume Import Risks:**

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Memory Exhaustion** | Low | High | Current 8GB provides 23x headroom |
| **NetSuite Rate Limits** | Medium | Medium | Built-in delays and batch sizing |
| **Database Locks** | Low | Medium | Individual item tracking, short transactions |
| **Network Timeouts** | Medium | Low | Retry logic and circuit breakers |
| **Process Crashes** | Low | High | Auto scaling and health checks |

### **Recommended Safeguards:**
1. **Dry Run Testing**: Always test large imports with `dryRun=true`
2. **Incremental Imports**: Break 10K+ items into multiple jobs
3. **Off-Peak Processing**: Schedule large imports during low usage
4. **Backup Strategy**: Maintain job status for recovery
5. **Monitoring Alerts**: Set up CloudWatch alarms for resource usage

---

## üìã **IMPLEMENTATION CHECKLIST FOR 6000+ ITEMS**

### **Pre-Import Validation:**
- [ ] Verify CSV file size <50MB
- [ ] Confirm vendor mappings exist in database
- [ ] Test with small batch first (100-500 items)
- [ ] Monitor system resources during test

### **Production Import Process:**
- [ ] Use optimized batch settings (`batchSize=20, delayMs=1000`)
- [ ] Monitor CloudWatch metrics during processing
- [ ] Track job progress via API endpoint
- [ ] Verify NetSuite item creation success rate

### **Post-Import Verification:**
- [ ] Confirm 100% success rate in job status
- [ ] Spot-check created items in NetSuite
- [ ] Verify all custom fields populated correctly
- [ ] Clean up any failed items if necessary

---

## üéâ **CONCLUSION**

**Your current AWS infrastructure is PERFECTLY configured for 6000 item NetSuite imports:**

- ‚úÖ **No hardware upgrades needed**
- ‚úÖ **Existing t4g.large instances are optimal**
- ‚úÖ **System tested and validated with 100% success rate**
- ‚úÖ **Built-in safeguards for large-scale processing**

The automated NetSuite CSV import system is **production-ready** for enterprise-scale operations! üöÄ

---

## üìö **REFERENCE LINKS**

- [NetSuite Integration Specification](./ai-specs/spec-NetSuite-Item-Creation.md)
- [AWS Infrastructure Guide](../aws-infrastructure/AWS-DEPLOYMENT-GUIDE.md)
- [CSV Export Documentation](./NetSuite-Integrations/OPMS-NetSuite-Sync-Implementation-Guide.md)
- [Testing Standards](./ai-specs/development/test-standards-and-conventions.md)
